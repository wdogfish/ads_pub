{"metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# DBSCAN"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["customers_ml_data = pd.read_csv('data/online_retail_afterEDA.csv', index_col='CustomerID')\n", "non_binary_cols = [\n", "    'balance', 'max_spent', 'mean_spent', \n", "    'min_spent', 'n_orders','total_items', \n", "    'total_refunded', 'total_spent' ]\n", "\n", "customers = customers_ml_data[non_binary_cols]\n", "Xscores = pd.read_csv('data/pca_scores.csv', index_col='CustomerID')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This function generates a pairplot enhanced with the result of k-means\n", "def pairplot_cluster(df, cols, cluster_assignment):\n", "    \"\"\"\n", "    Input\n", "        df, dataframe that contains the data to plot\n", "        cols, columns to consider for the plot\n", "        cluster_assignments, cluster asignment returned \n", "        by the clustering algorithm\n", "    \"\"\"\n", "    # seaborn will color the samples according to the column cluster\n", "    df['cluster'] = cluster_assignment \n", "    sns.pairplot(df, vars=cols, hue='cluster')\n", "    df.drop('cluster', axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are globular. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster. \n", "\n", "Summary of the Algorithm:\n", "\n", "- starts with an arbitrary starting point and retrieved all the points in the radius of distance `eps` from it \n", "    - if the radius contains `min_samples` points, start a cluster\n", "      - add all the points in the radius of distance `eps` to the cluster and their `eps` neighbors.\n", "      - continue expanding the cluster iterating on the the procedure on all the neighbors\n", "    - otherwise mark it as noise/outlier\n", "\n", "Sklearn implementation doc: http://scikit-learn.org/stable/modules/clustering.html#dbscan\n", "\n", "Animated DBSCAN: http://www.naftaliharris.com/blog/visualizing-dbscan-clustering/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Learning Activity - A starting value for eps\n", "\n", "Measure the distance of each point to its closest neighbor using the function `sklearn.metrics.pairwise.pairwise_distances` (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) and plot the distribution of the distances."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics.pairwise import pairwise_distances\n", "\n", "# Compute all the pairwise distances\n", "all_distances = pairwise_distances(customers, metric='euclidean')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# compute the distance of each point to its closest neighbor\n", "neig_distances = [np.min(row[np.nonzero(row)]) for row in all_distances]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the distances\n", "plt.hist(neig_distances, bins=50)\n", "plt.xlabel('Distance from closest sample')\n", "plt.ylabel('Occurrences')\n", "plt.axis([0,1.5,0,2500])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The distribution of the distance will help us choose a starting point for `eps`. We see that it's very likely that a point as at least one neighbour in a radius of 0.15 and that only very few point have one at distance over 1.0. Since we want that a core point has more than one point in is `eps`-neighborhood we can start picking `eps` on the right tail of the distribution."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Learning Activity - Applying DBSCAN\n", "\n", "Cluster the customer data with DBSCAN and visualise the results in the subspaces used for the other algorithms."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import DBSCAN\n", "\n", "# Apply DBSCAN setting eps to 1.0 and min samples to 8 (say)\n", "\n", "db = DBSCAN(eps=1.0, min_samples=8)\n", "cluster_assignment = db.fit_predict(customers)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Display how many clusters were found\n", "\n", "clusters_found = np.unique(cluster_assignment)\n", "print ('Clusters found', len(clusters_found))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can then visualise this"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualise the clusters using pairplot_cluster()\n", "pairplot_cluster(customers, ['mean_spent', 'max_spent'], cluster_assignment)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DBSCAN clustered all the points in one big cluster and marked as outiers all the points that are not in dense areas."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning Activity -  How many clusters with DBSCAN?\n", "\n", "Vary `eps` and `min_samples` and study how the number of clusters varies as result. This way we'll have an idea of how many cluster we get varying the parameters. This can help us choose the parameters if we already have an idea of how many clusters we want to create.\n", "\n", "Warning, if you cover a grid of points, it may take a while to finish, don't put too many points!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# WARNING this may take a couple of minutes to finish!\n", "eps = np.linspace(.5, 10.0, 5)\n", "mins = np.arange(5, 50, 5)\n", "Z = np.zeros((len(eps), len(mins)))\n", "\n", "for i, e in enumerate(eps):\n", "    for j, m in enumerate(mins):\n", "        db   = DBSCAN(eps=e, min_samples=m)\n", "        pred = db.fit_predict(customers)\n", "        clusters_found = len(np.unique(pred))\n", "        Z[i,j] = clusters_found"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualise this using a heatmap\n", "plt.figure(figsize=(10, 10))\n", "sns.heatmap(Z, cmap='RdBu', center=0, annot=True);\n", "plt.xticks(np.arange(Z.shape[1]), mins)\n", "plt.xlabel('min_samples')\n", "plt.yticks(np.arange(Z.shape[0]), ['%0.2f' % x for x in eps])\n", "plt.ylabel('eps')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning activity - Compute the silhouette score of the DBSCAN cluster\n", "\n", "Compute the silhouette score of the clusters made with DBSCAN and compare it with the silhouette score achieved with K-Means."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute the silhouette score of DBSCAN\n", "print(silhouette_score(customers, cluster_assignment))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Test Activity - Run DBSCAN on the dataset obtained with PCA\n", "\n", "Run DBSCAN on the first 3 principal components in `Xscores`.\n", "\n", "Tweak the parameters to achieve about 5 clusters as result."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run DBSCAN on the first 3 principal components\n", "\n", "db = DBSCAN(eps=.9, min_samples=5)\n", "cluster_assignment = db.fit_predict(Xscores[['PC1', 'PC2', 'PC3']])\n", "\n", "clusters_found = np.unique(cluster_assignment)\n", "print ('Clusters found', len(clusters_found))\n", "\n", "pairplot_cluster(Xscores, ['PC1', 'PC2', 'PC3'], cluster_assignment )\n"]}]}